{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEfbrOXKXkCp"
   },
   "outputs": [],
   "source": [
    "PRO_ISRAELI = [\n",
    "'israel', 'israeli', 'zionism', 'idf', 'jewish state', 'jerusalem', 'netanyahu', 'jewish', 'hebrew', 'zionist entity'\n",
    "]\n",
    "PRO_PALESTINIAN = [\n",
    "    'hezbollah', 'sinwar', 'palestine', 'hamas', 'gaza', 'west bank', 'plo', 'abbas', 'intifada', 'nakba'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Trrb0Z2ISuGK",
    "outputId": "dce48571-dcac-4048-90b1-84233359dcbb"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "30125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "# Load the Excel file\n",
    "def process_text():\n",
    "  file_path = \"posts_first_targil.xlsx\"\n",
    "  df = pd.read_excel(file_path, sheet_name=None)\n",
    "\n",
    "  # Initialize the new DataFrame\n",
    "  df_sentence = pd.DataFrame(columns=['sheet_name', 'id', 'sentence'])\n",
    "\n",
    "  # Process each sheet in the Excel file\n",
    "  for sheet_name, data in df.items():\n",
    "      for index, row in data.iterrows():\n",
    "          # Combine text based on the sheet\n",
    "          if sheet_name == 'A-J':\n",
    "              combined_text = \" \".join(str(row[col]) for col in ['title', 'sub_title', 'Body Text'] if col in row and pd.notna(row[col]))\n",
    "          else:\n",
    "              combined_text = \" \".join(str(row[col]) for col in ['title', 'Body Text'] if col in row and pd.notna(row[col]))\n",
    "\n",
    "          # Split the combined text into sentences\n",
    "          sentences = [sentence.strip() for sentence in re.split(r'[.!?]', combined_text) if sentence.strip()]\n",
    "\n",
    "          # Append sentences to the new DataFrame\n",
    "          for sentence in sentences:\n",
    "              df_sentence = pd.concat([df_sentence, pd.DataFrame({ 'sheet_name': [sheet_name], 'id': [index], 'sentence': [sentence]})], ignore_index=True)\n",
    "  df_sentence.head()\n",
    "  return df_sentence\n",
    "\n",
    "data = process_text()\n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bXz8Ix40YLa-",
    "outputId": "e6628ead-7832-4f3c-8ef5-e6f61f06b265"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8909\n"
     ]
    }
   ],
   "source": [
    "def extract_sentences(df, pro_israeli_words, pro_palestinian_words):\n",
    "    extracted = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        doc_id = row['id']\n",
    "\n",
    "        sentence = row['sentence'].strip().lower()\n",
    "        sheet_name = row['sheet_name']\n",
    "        # Check for pro-Israeli and pro-Palestinian keywords\n",
    "        is_pro_israeli = any(word in sentence for word in pro_israeli_words)\n",
    "        is_pro_palestinian = any(word in sentence for word in pro_palestinian_words)\n",
    "\n",
    "        # Determine the type based on keywords\n",
    "        if is_pro_israeli and not is_pro_palestinian:\n",
    "            extracted.append((doc_id,sheet_name, sentence, 'pro-israeli'))\n",
    "        elif is_pro_palestinian and not is_pro_israeli:\n",
    "            extracted.append((doc_id,sheet_name, sentence, 'pro-palestinian'))\n",
    "\n",
    "    return pd.DataFrame(extracted, columns=['id','sheet_name', 'sentence', 'lebal'])\n",
    "\n",
    "data = extract_sentences(data,PRO_ISRAELI,PRO_PALESTINIAN)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jsmL4PleZzyP"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "# Run sentiment analysis using Hugging Face models\n",
    "\n",
    "def sentiment_models():\n",
    "    model_paths = {\n",
    "        'model1': \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "        'model2': \"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "        'model3': \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n",
    "        'model4': \"siebert/sentiment-roberta-large-english\",\n",
    "        'model5': \"lxyuan/distilbert-base-multilingual-cased-sentiments-student\",\n",
    "        'model6': \"finiteautomata/bertweet-base-sentiment-analysis\",\n",
    "        'model7': \"j-hartmann/sentiment-roberta-large-english-3-classes\"\n",
    "    }\n",
    "\n",
    "    sent_models = {}\n",
    "    for name, path in model_paths.items():\n",
    "        sent_models[name] = pipeline(\"sentiment-analysis\", model=path,device=0)\n",
    "    return sent_models\n",
    "models = sentiment_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yM-WTL7yGLo"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def map_sentiment_to_context(sentiment_label, score, sentence_context):\n",
    "    \"\"\"Map sentiment label to pro-Israeli/pro-Palestinian context.\"\"\"\n",
    "\n",
    "    # Normalize sentiment label\n",
    "    sentiment = sentiment_label.lower()\n",
    "\n",
    "    # Neutral sentiment handling\n",
    "    if 'neutral' in sentiment:\n",
    "        return 'NEUTRAL'\n",
    "\n",
    "    # Map sentiment based on sentence context\n",
    "    if sentence_context == 'pro-israeli':\n",
    "        return 'POS' if sentiment in ['positive', 'pos'] else 'NEG'\n",
    "    elif sentence_context == 'pro-palestinian':\n",
    "        return 'POS' if sentiment in ['positive', 'pos'] else 'NEG'\n",
    "\n",
    "    # Default to NEUTRAL if no match\n",
    "    return 'NEUTRAL'\n",
    "\n",
    "\n",
    "def evaluate_sentence_with_models(sentence, context_type, models, tokenizer_name=\"bert-base-uncased\"):\n",
    "    \"\"\"\n",
    "    Evaluate a sentence using multiple models, splitting it into chunks if necessary.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence to evaluate.\n",
    "        context_type (str): The context type (e.g., 'pro-israeli', 'pro-palestinian').\n",
    "        models (dict): A dictionary of model names and their instances.\n",
    "        tokenizer_name (str): The tokenizer to use for token length calculation.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of model results including scores and labels.\n",
    "    \"\"\"\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    # Tokenize the sentence and check its length\n",
    "    tokenized_sentence = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "    if len(tokenized_sentence) > 128:\n",
    "        count += count\n",
    "    if len(tokenized_sentence) <= 512:\n",
    "        # Sentence fits within token limits\n",
    "        return _evaluate_single_sentence(sentence, context_type, models)\n",
    "    else:\n",
    "        # Split the sentence into smaller chunks\n",
    "        chunks = _split_into_chunks(sentence, tokenizer, max_length=512)\n",
    "        aggregated_results = {}\n",
    "\n",
    "        for chunk in chunks:\n",
    "            chunk_results = _evaluate_single_sentence(chunk, context_type, models)\n",
    "            for key, value in chunk_results.items():\n",
    "                if key in aggregated_results:\n",
    "                    aggregated_results[key].append(value)\n",
    "                else:\n",
    "                    aggregated_results[key] = [value]\n",
    "\n",
    "        # Aggregate results across chunks\n",
    "        return _aggregate_chunk_results(aggregated_results)\n",
    "\n",
    "\n",
    "def _evaluate_single_sentence(sentence, context_type, models):\n",
    "    \"\"\"\n",
    "    Evaluate a single sentence using multiple models.\n",
    "    \"\"\"\n",
    "    evaluation_results = {}\n",
    "\n",
    "    for model_name, model_instance in models.items():\n",
    "        try:\n",
    "            prediction = model_instance(sentence)[0]\n",
    "            evaluation_results[f\"{model_name}_score\"] = prediction['score']\n",
    "            evaluation_results[f\"{model_name}_label\"] = map_sentiment_to_context(\n",
    "                prediction['label'], prediction['score'], context_type\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error with model '{model_name}' for sentence: '{sentence[:50]}...' Error: {e}\")\n",
    "            evaluation_results[f\"{model_name}_score\"] = None\n",
    "            evaluation_results[f\"{model_name}_label\"] = None\n",
    "\n",
    "    return evaluation_results\n",
    "\n",
    "\n",
    "def _split_into_chunks(sentence, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Split a sentence into chunks that fit within the token limit.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The sentence to split.\n",
    "        tokenizer: The tokenizer instance.\n",
    "        max_length (int): The maximum number of tokens per chunk.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of sentence chunks.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for word in words:\n",
    "        # Add word to the current chunk and check its token length\n",
    "        current_chunk.append(word)\n",
    "        if len(tokenizer.encode(\" \".join(current_chunk), add_special_tokens=True)) > max_length:\n",
    "            # Remove the last word and finalize the current chunk\n",
    "            current_chunk.pop()\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "\n",
    "    # Add the final chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def _aggregate_chunk_results(results):\n",
    "    \"\"\"\n",
    "    Aggregate results across multiple chunks by averaging scores and determining majority labels.\n",
    "    \"\"\"\n",
    "    aggregated = {}\n",
    "\n",
    "    for key, values in results.items():\n",
    "        if '_score' in key:\n",
    "            # Average scores\n",
    "            aggregated[key] = sum(values) / len(values)\n",
    "        elif '_label' in key:\n",
    "            # Majority label\n",
    "            aggregated[key] = max(set(values), key=values.count)\n",
    "\n",
    "    return aggregated\n",
    "\n",
    "def determine_majority_sentiment(row):\n",
    "    \"\"\"Determine the majority sentiment label across multiple models.\"\"\"\n",
    "    sentiment_labels = [value for key, value in row.items() if '_label' in key and value is not None]\n",
    "\n",
    "    if not sentiment_labels:\n",
    "        return 'UNKNOWN'\n",
    "\n",
    "    # Determine majority label\n",
    "    majority_label = max(set(sentiment_labels), key=sentiment_labels.count)\n",
    "    return majority_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Tz_Fd23mQuo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "\n",
    "import torch\n",
    "# Process each sentence\n",
    "results = []\n",
    "for idx, row in tqdm(data.iterrows(), total=len(data)):\n",
    "    result = {\n",
    "        'newspaper': row['sheet_name'].split('_')[0],\n",
    "        'article_id': row['id'],\n",
    "        'sentence': row['sentence'],\n",
    "        'type': row['lebal']\n",
    "    }\n",
    "\n",
    "    # Add model predictions\n",
    "    result.update(evaluate_sentence_with_models(row['sentence'], row['lebal'], models))\n",
    "\n",
    "    # Add to results\n",
    "    results.append(result)\n",
    "print(count)\n",
    "# Create final DataFrame\n",
    "output_df = pd.DataFrame(results)\n",
    "\n",
    "# Add majority decision\n",
    "output_df['majority_decision'] = output_df.apply(determine_majority_sentiment, axis=1)\n",
    "\n",
    "# Calculate average score for majority decision\n",
    "score_columns = [col for col in output_df.columns if '_score' in col]\n",
    "output_df['avg_majority_score'] = output_df[score_columns].mean(axis=1)\n",
    "# Save to Excel\n",
    "output_df.to_excel('sentiment.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xoPn7qP8S8NO",
    "outputId": "cbe339ea-a14c-4cf4-c516-9f211c203df4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "\n",
    "import torch\n",
    "\n",
    "# Now you can use CUDA and device-side assertions will be enabled.\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LroPezZoiWit",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def determine_dominant_label(label_counts):\n",
    "    \"\"\"\n",
    "    Determines the label with the highest count.\n",
    "    If there is a tie, returns 'N/A'.\n",
    "    \"\"\"\n",
    "    # Find the maximum count in the dictionary\n",
    "    max_count = max(label_counts.values())\n",
    "\n",
    "    # Identify all labels with the maximum count\n",
    "    dominant_labels = [label for label, count in label_counts.items() if count == max_count]\n",
    "\n",
    "    # Return the dominant label if there is only one, otherwise return 'N/A' for ties\n",
    "    return dominant_labels[0] if len(dominant_labels) == 1 else 'N/A'\n",
    "\n",
    "# Creating a list to store results for each paper\n",
    "papers_results = []\n",
    "for paper_id in output_df['newspaper'].unique():\n",
    "    paper_data = output_df[output_df['newspaper'] == paper_id]\n",
    "\n",
    "    articles_data = []\n",
    "\n",
    "    for article_idx in paper_data['article_id'].unique():\n",
    "        article_data = paper_data[paper_data['article_id'] == article_idx]\n",
    "\n",
    "        label_counts = {'POS-I': 0, 'POS-P': 0, 'NUT': 0}\n",
    "        score_sums = {'POS-I': 0, 'POS-P': 0, 'NUT': 0}\n",
    "\n",
    "        for idx, row in article_data.iterrows():\n",
    "            if (row['majority_decision'] == 'POS' and row['type'] == 'pro-israeli') or (row['majority_decision'] == 'NEG' and row['type'] == 'pro-palestinian'):\n",
    "                label_counts['POS-I'] += 1\n",
    "                score_sums['POS-I'] += row['avg_majority_score']\n",
    "            elif (row['majority_decision'] == 'POS' and row['type'] == 'pro-palestinian') or (row['majority_decision'] == 'NEG' and row['type'] == 'pro-israeli'):\n",
    "                label_counts['POS-P'] += 1\n",
    "                score_sums['POS-P'] += row['avg_majority_score']\n",
    "            elif row['majority_decision'] == 'NEUTRAL':\n",
    "                label_counts['NUT'] += 1\n",
    "                score_sums['NUT'] += row['avg_majority_score']\n",
    "\n",
    "        dominant_label = determine_dominant_label(label_counts)\n",
    "        articles_data.append({\n",
    "            'article_index': article_idx,\n",
    "            'dominant_label': dominant_label,\n",
    "            'average_score': score_sums[dominant_label] / label_counts[dominant_label] if dominant_label != 'N/A' else 1,\n",
    "        })\n",
    "\n",
    "    papers_results.append((pd.DataFrame(articles_data), paper_id))\n",
    "\n",
    "# Save to Excel with updated sheet names\n",
    "with pd.ExcelWriter('updated_results.xlsx') as writer:\n",
    "    for article_df, paper in papers_results:\n",
    "        article_df.to_excel(writer, sheet_name=str(paper), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2-4rHIf2iXmO",
    "outputId": "efd9d34d-4850-45e2-98af-20c3599e9270"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "paper: A-J label: POS-P score: 0.7497990552337218\n",
      "paper: BBC label: POS-P score: 0.7481500840890505\n",
      "paper: J-P label: POS-P score: 0.7474796354869099\n",
      "paper: NY-T label: POS-P score: 0.744396989673225\n"
     ]
    }
   ],
   "source": [
    "for df, sheet_name in papers_results:\n",
    "    pro_classification = df['dominant_label'].value_counts()\n",
    "    pro_classification = pro_classification.to_dict()\n",
    "    pro_classification['N/A'] = 0\n",
    "\n",
    "    decided_pro = determine_dominant_label(pro_classification)\n",
    "    score = df[df['dominant_label'] == decided_pro]['average_score'].mean() if decided_pro != 'N/A' else 'N/A'\n",
    "    print(f'paper: {sheet_name} label: {decided_pro} score: {score}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
