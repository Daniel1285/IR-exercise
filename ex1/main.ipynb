{
  "cells": [
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2024-12-03T11:07:57.318940Z",
          "start_time": "2024-12-03T11:07:57.313420Z"
        },
        "id": "initial_id"
      },
      "source": [
        "import re"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T11:29:15.295191Z",
          "start_time": "2024-12-03T11:29:14.159771Z"
        },
        "id": "8f6169cecdc5faef",
        "outputId": "2794f9a6-0b4f-425b-9964-2516c79ba15b"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "input_file = \"posts_first_targil.xlsx\"\n",
        "\n",
        "# Read the Excel file with multiple sheets\n",
        "df = pd.read_excel(input_file, sheet_name=None)\n",
        "\n",
        "for sheet_name, data in df.items():\n",
        "    print(f\"Sheet name: {sheet_name} Headlines:, {list(data.columns)}\")"
      ],
      "id": "8f6169cecdc5faef",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sheet name: A-J Headlines:, ['sub_title', 'date', 'Newspaper', 'Body Text', 'title']\n",
            "Sheet name: BBC Headlines:, ['date', 'Newspaper', 'Body Text', 'title']\n",
            "Sheet name: J-P Headlines:, ['date', 'Newspaper', 'Body', 'title']\n",
            "Sheet name: NY-T Headlines:, ['date', 'Newspaper', 'Body Text', 'title']\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "b7b6eb676ad2fff4"
      },
      "cell_type": "markdown",
      "source": [],
      "id": "b7b6eb676ad2fff4"
    },
    {
      "metadata": {
        "id": "fb361094312c4d46"
      },
      "cell_type": "raw",
      "source": [
        "JP title update like the other files"
      ],
      "id": "fb361094312c4d46"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T11:29:18.002644Z",
          "start_time": "2024-12-03T11:29:17.989803Z"
        },
        "id": "64f4ca6cf3c34771",
        "outputId": "322b4407-6121-48cb-9e74-a517638b0d6b"
      },
      "cell_type": "code",
      "source": [
        "if \"J-P\" in df:\n",
        "    df[\"J-P\"].rename(columns={\"Body\": \"Body Text\"}, inplace=True)\n",
        "\n",
        "# Verify the change\n",
        "for sheet_name, data in df.items():\n",
        "    print(f\"Sheet name: {sheet_name} Headlines:, {list(data.columns)}\")"
      ],
      "id": "64f4ca6cf3c34771",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sheet name: A-J Headlines:, ['sub_title', 'date', 'Newspaper', 'Body Text', 'title']\n",
            "Sheet name: BBC Headlines:, ['date', 'Newspaper', 'Body Text', 'title']\n",
            "Sheet name: J-P Headlines:, ['date', 'Newspaper', 'Body Text', 'title']\n",
            "Sheet name: NY-T Headlines:, ['date', 'Newspaper', 'Body Text', 'title']\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "cfed82032f5b5b41"
      },
      "cell_type": "raw",
      "source": [],
      "id": "cfed82032f5b5b41"
    },
    {
      "metadata": {
        "id": "754ab94f6bb69555"
      },
      "cell_type": "markdown",
      "source": [
        "**Function to clean the data text**"
      ],
      "id": "754ab94f6bb69555"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T11:12:45.033741Z",
          "start_time": "2024-12-03T11:12:45.020935Z"
        },
        "id": "15e46b2d092cedee"
      },
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    regx = r\"((?<!\\w)[^\\s\\w]|[^\\s\\w](?!\\w))\"\n",
        "    dot_pattern = r\"(?<!\\w)([a-zA-Z]{2,})\\.([a-zA-Z]{2,})(?!\\w)\"\n",
        "    clean_t = re.sub(regx, r\" \\1 \", text)\n",
        "    clean_t = re.sub(dot_pattern, r\"\\1 . \\2\", clean_t)\n",
        "    return re.sub(r\"\\s+\", \" \", clean_t).strip()"
      ],
      "id": "15e46b2d092cedee",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "229fe1d446584279"
      },
      "cell_type": "raw",
      "source": [],
      "id": "229fe1d446584279"
    },
    {
      "metadata": {
        "id": "bbded466b605420a"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Part 2: Functions for processing data by lemmatize the text**"
      ],
      "id": "bbded466b605420a"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T11:12:50.268543Z",
          "start_time": "2024-12-03T11:12:49.420899Z"
        },
        "id": "a5bbc264e54c21ca"
      },
      "cell_type": "code",
      "source": [
        "# Load spaCy's language model\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "id": "a5bbc264e54c21ca",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T11:12:59.997500Z",
          "start_time": "2024-12-03T11:12:59.981336Z"
        },
        "id": "7bcd1ad78e7bc9f0"
      },
      "cell_type": "code",
      "source": [
        "def lemmatize_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    doc = nlp(text)\n",
        "    return \" \".join([token.lemma_ for token in doc])"
      ],
      "id": "7bcd1ad78e7bc9f0",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "c718ae6806f75031"
      },
      "cell_type": "raw",
      "source": [],
      "id": "c718ae6806f75031"
    },
    {
      "metadata": {
        "id": "412a36025083441f"
      },
      "cell_type": "markdown",
      "source": [
        "**Creating the processed files:**\n",
        "\n",
        "*A.* clean_file.xlsx\n",
        "\n",
        "*B.* lemma_file.xlsx\n",
        "\n",
        "**By each of the functions:**\n",
        "\n",
        "*A.* lemmatize_text()\n",
        "\n",
        "*B.* clean_text()"
      ],
      "id": "412a36025083441f"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T11:16:03.656612Z",
          "start_time": "2024-12-03T11:13:04.956031Z"
        },
        "id": "12517a4f8eb156b4",
        "outputId": "36c942f8-9160-4517-d4c6-81bd6000268c"
      },
      "cell_type": "code",
      "source": [
        "clean_sheets = {}\n",
        "lemma_sheets = {}\n",
        "for sheet_name, data in df.items():\n",
        "    # Apply clean_text to all string columns in the DataFrame\n",
        "    processed_clean_df = data.map(clean_text)\n",
        "    clean_sheets[sheet_name] = processed_clean_df\n",
        "    processed_lemma_df = data.map(lemmatize_text)\n",
        "    lemma_sheets[sheet_name] = processed_lemma_df\n",
        "\n",
        "# Save each processed sheet to a separate Excel file\n",
        "output_clean_file = \"output_files/clean_file.xlsx\"\n",
        "with pd.ExcelWriter(output_clean_file) as writer:\n",
        "    for sheet_name, processed_df in clean_sheets.items():\n",
        "        processed_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        "output_lemma_file = \"output_files/lemma_file.xlsx\"\n",
        "with pd.ExcelWriter(output_lemma_file) as writer:\n",
        "    for sheet_name, processed_df in lemma_sheets.items():\n",
        "        processed_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "\n",
        "print(f\"Processed clean Excel file saved as: {output_clean_file}\")\n",
        "print(f\"Processed lemma Excel file saved as: {output_lemma_file}\")\n"
      ],
      "id": "12517a4f8eb156b4",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed clean Excel file saved as: output_files/clean_file.xlsx\n",
            "Processed lemma Excel file saved as: output_files/lemma_file.xlsx\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "fbe0a162d3051463"
      },
      "cell_type": "raw",
      "source": [],
      "id": "fbe0a162d3051463"
    },
    {
      "metadata": {
        "id": "dac0f8d4a549ae46"
      },
      "cell_type": "markdown",
      "source": [
        "***Part 3: using TF-IDF BM25/Okapi (words_file or lemma_files)***"
      ],
      "id": "dac0f8d4a549ae46"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T11:29:23.364041Z",
          "start_time": "2024-12-03T11:29:23.350047Z"
        },
        "id": "abfbd881956b975c"
      },
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n"
      ],
      "id": "abfbd881956b975c",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T11:46:19.198624Z",
          "start_time": "2024-12-03T11:45:07.353534Z"
        },
        "id": "e48d50601fd82b5b"
      },
      "cell_type": "code",
      "source": [
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize the set of English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define the function to filter out stopwords from text\n",
        "def filter_stopwords(text):\n",
        "    tokens = text.split()  # Split the input into tokens\n",
        "    return [token.lower() for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "# File paths\n",
        "input_file = \"output_files/clean_file.xlsx\"\n",
        "words_file = \"output_files/words_file.xlsx\"  # File containing the words to use in BM25\n",
        "output_folder = \"bm25/clean\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Load the specific words from the file\n",
        "words_df = pd.read_excel(words_file, header=None)\n",
        "words_to_include = set(words_df[0].str.lower())  # Convert to lowercase to ensure case-insensitive matching\n",
        "\n",
        "# Read the input Excel file\n",
        "clean_df = pd.read_excel(input_file, sheet_name=None)\n",
        "\n",
        "# Process each sheet\n",
        "for sheet_name, data in clean_df.items():\n",
        "    print(f\"Processing sheet: {sheet_name}\")\n",
        "\n",
        "    # Construct corpus by removing stopwords and combining text fields\n",
        "    if sheet_name == \"A-J\":\n",
        "        documents = [\n",
        "            filter_stopwords(f'{record[\"title\"]} {record[\"sub_title\"]} {record[\"Body Text\"]}')\n",
        "            for _, record in data.iterrows()\n",
        "        ]\n",
        "    else:\n",
        "        documents = [\n",
        "            filter_stopwords(f'{record[\"title\"]} {record[\"Body Text\"]}')\n",
        "            for _, record in data.iterrows()\n",
        "        ]\n",
        "\n",
        "    # Filter documents to only include the specified words\n",
        "    filtered_documents = [\n",
        "        [word for word in doc if word in words_to_include]\n",
        "        for doc in documents\n",
        "    ]\n",
        "\n",
        "    # Create BM25 model\n",
        "    bm25_model = BM25Okapi(filtered_documents)\n",
        "\n",
        "    # Create a BM25 matrix for the specific words\n",
        "    bm25_matrix = []\n",
        "    for doc_index, document in enumerate(filtered_documents):\n",
        "        doc_scores = {word: bm25_model.get_scores([word])[doc_index] for word in words_to_include}\n",
        "        bm25_matrix.append(doc_scores)\n",
        "\n",
        "    # Convert BM25 matrix to a DataFrame\n",
        "    bm25_df = pd.DataFrame(bm25_matrix).fillna(0)  # Fill NaN with 0 for words with no score in a document\n",
        "    bm25_df.insert(0, \"DocumentIndex\", range(len(filtered_documents)))  # Add document indices as the first column\n",
        "\n",
        "    # Save the BM25 scores to an Excel file\n",
        "    output_file = os.path.join(output_folder, f\"bm25_{sheet_name}.xlsx\")\n",
        "    bm25_df.to_excel(output_file, index=False)\n",
        "    print(f\"BM25 matrix for sheet '{sheet_name}' saved to {output_file}\")"
      ],
      "id": "e48d50601fd82b5b",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "b96a8d38d87ab409"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [],
      "id": "b96a8d38d87ab409"
    },
    {
      "metadata": {
        "id": "683891a07880b2f0"
      },
      "cell_type": "raw",
      "source": [],
      "id": "683891a07880b2f0"
    },
    {
      "metadata": {
        "id": "3a72524744be6885"
      },
      "cell_type": "markdown",
      "source": [
        "PART 4 - Word2Vec (glove or w2v) without IDF"
      ],
      "id": "3a72524744be6885"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T13:33:33.113419Z",
          "start_time": "2024-12-03T13:33:30.721134Z"
        },
        "id": "f65ee82a87e5ba4f"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import gensim.downloader as api\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.data import find\n",
        "import csv\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "id": "f65ee82a87e5ba4f",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "de1d94cd4058a16e"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.data import find\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "# File paths\n",
        "input_file = \"/content/output/lemma_file.xlsx\"  # Replace with your Excel file path\n",
        "output_file = \"/content/output/glove_lemma_withoutIdf_withStopWords.xlsx\"\n",
        "\n",
        "df = pd.read_excel(input_file, sheet_name=None)\n",
        "\n",
        "if \"J-P\" in df:\n",
        "    df[\"J-P\"].rename(columns={\"Body\": \"Body Text\"}, inplace=True)\n",
        "\n",
        "# Load GloVe vectors via gensim downloader\n",
        "try:\n",
        "    glove_model = api.load(\"glove-wiki-gigaword-300\")  # 300-dimensional GloVe vectors\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\d+\", \"\", text)  # Remove digits and dates\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
        "    tokens = word_tokenize(text)  # Tokenize the text\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# Process each sheet\n",
        "results = []\n",
        "\n",
        "for sheet_name, data in df.items():\n",
        "    for index, row in data.iterrows():\n",
        "        # Combine text from relevant columns\n",
        "        if sheet_name == 'A-J':\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'sub_title', 'Body Text'] if pd.notna(row[col]))\n",
        "        else:\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'Body Text'] if pd.notna(row[col]))\n",
        "\n",
        "        # Preprocess text and get tokens\n",
        "        tokens = preprocess_text(combined_text)\n",
        "        # Extract vectors for each word\n",
        "        vectors = []\n",
        "        for word in tokens:\n",
        "            if word in glove_model:\n",
        "                vectors.append(glove_model[word])\n",
        "\n",
        "        # If there are word vectors for the document, compute the average\n",
        "        if vectors:\n",
        "            avg_vector = np.mean(vectors, axis=0)\n",
        "            results.append([sheet_name, index] + avg_vector.tolist())\n",
        "\n",
        "# Save results to a CSV file\n",
        "header = [\"Sheet\", \"RowIndex\"] + [f\"Dim{i}\" for i in range(glove_model.vector_size)]\n",
        "with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(header)\n",
        "    writer.writerows(results)\n",
        "\n",
        "print(f\"Word vectors saved to {output_file}\")\n"
      ],
      "id": "de1d94cd4058a16e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec with IDF"
      ],
      "metadata": {
        "id": "qcOxWoPZ4uDk"
      },
      "id": "qcOxWoPZ4uDk"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import gensim.downloader as api\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "import csv\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "WLjkA_sm4yvD"
      },
      "id": "WLjkA_sm4yvD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "input_file = \"/content/output/lemma_file.xlsx\"\n",
        "output_file = \"/content/output/glove_lemma_withIDF_withoutStopWords.csv\"\n",
        "\n",
        "df = pd.read_excel(input_file, sheet_name=None)\n",
        "\n",
        "if \"J-P\" in df:\n",
        "    df[\"J-P\"].rename(columns={\"Body\": \"Body Text\"}, inplace=True)\n",
        "\n",
        "try:\n",
        "    glove_model = api.load(\"glove-wiki-gigaword-300\")  # 300-dimensional GloVe vectors\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\d+\", \"\", text)  # Remove digits and dates\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
        "    tokens = word_tokenize(text)  # Tokenize the text\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    return [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
        "\n",
        "def calculate_idf(corpus):\n",
        "    vectorizer = TfidfVectorizer(use_idf=True, stop_words=\"english\")\n",
        "    vectorizer.fit(corpus)\n",
        "    idf_dict = defaultdict(lambda: 0)\n",
        "    for word, idf in zip(vectorizer.get_feature_names_out(), vectorizer.idf_):\n",
        "        idf_dict[word] = idf\n",
        "    return idf_dict\n",
        "\n",
        "corpus = []\n",
        "for sheet_name, data in df.items():\n",
        "    for index, row in data.iterrows():\n",
        "        if sheet_name == 'A-J':\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'sub_title', 'Body Text'] if pd.notna(row[col]))\n",
        "        else:\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'Body Text'] if pd.notna(row[col]))\n",
        "        corpus.append(combined_text)\n",
        "\n",
        "idf_dict = calculate_idf(corpus)\n",
        "\n",
        "\n",
        "results = []\n",
        "for sheet_name, data in df.items():\n",
        "    for index, row in data.iterrows():\n",
        "        # Combine text from relevant columns\n",
        "        if sheet_name == 'A-J':\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'sub_title', 'Body Text'] if pd.notna(row[col]))\n",
        "        else:\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'Body Text'] if pd.notna(row[col]))\n",
        "\n",
        "        # Preprocess text and get tokens\n",
        "        tokens = preprocess_text(combined_text)\n",
        "        vectors = []\n",
        "        for word in tokens:\n",
        "            if word in glove_model:\n",
        "                vector = glove_model[word]\n",
        "                idf_value = idf_dict[word]\n",
        "                vectors.append(vector * idf_value)\n",
        "\n",
        "        # If there are word vectors for the document, compute the average\n",
        "        if vectors:\n",
        "            avg_vector = np.mean(vectors, axis=0)\n",
        "            results.append([sheet_name, index] + avg_vector.tolist())\n",
        "\n",
        "# Save results to a CSV file\n",
        "header = [\"Sheet\", \"RowIndex\"] + [f\"Dim{i}\" for i in range(glove_model.vector_size)]\n",
        "with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(header)\n",
        "    writer.writerows(results)\n",
        "\n",
        "print(f\"Word vectors saved to {output_file}\")"
      ],
      "metadata": {
        "id": "rUlgGkHG42xF"
      },
      "id": "rUlgGkHG42xF",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bb8d4da48dfe42fc"
      },
      "cell_type": "markdown",
      "source": [
        "Part 5: doc2vec"
      ],
      "id": "bb8d4da48dfe42fc"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T13:34:04.461362Z",
          "start_time": "2024-12-03T13:34:04.441263Z"
        },
        "id": "da0176c45483930a"
      },
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
      ],
      "id": "da0176c45483930a",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "7f53f8cc076dd623"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "source_file = \"posts_first_targil.xlsx\"\n",
        "\n",
        "# Load source Excel file\n",
        "df = pd.read_excel(source_file, sheet_name=None)\n",
        "if \"J-P\" in df:\n",
        "    df[\"J-P\"].rename(columns={\"Body\": \"Body Text\"}, inplace=True)\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r\"\\d+\", \"\", text)  # Remove digits\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
        "    return word_tokenize(text)  # Tokenize the text\n",
        "\n",
        "# Prepare TaggedDocuments\n",
        "tagged_documents = []\n",
        "for sheet_name, data in df.items():\n",
        "    for index, row in data.iterrows():\n",
        "        if sheet_name == \"A-J\":\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'sub_title', 'Body Text'] if pd.notna(row[col]))\n",
        "        else:\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'Body Text'] if pd.notna(row[col]))\n",
        "\n",
        "        tokens = preprocess_text(combined_text)\n",
        "        tagged_documents.append(TaggedDocument(words=tokens, tags=[f\"{sheet_name}_{index}\"]))\n",
        "\n",
        "# Train Doc2Vec model\n",
        "model = Doc2Vec(vector_size=300, min_count=2, epochs=40, workers=4)\n",
        "model.build_vocab(tagged_documents)\n",
        "model.train(tagged_documents, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "# Save document vectors to CSV\n",
        "output_file = \"output_files/doc2vec_vectors.csv\"\n",
        "header = \"Sheet,RowIndex,\" + \",\".join([f\"Dim{i}\" for i in range(model.vector_size)])\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(header + \"\\n\")\n",
        "    for doc_id, doc in enumerate(tagged_documents):\n",
        "        # Extract sheet name and row index from doc.tags[0]\n",
        "        sheet, row_index = doc.tags[0].split(\"_\")\n",
        "        vector = model.dv[doc.tags[0]].tolist()\n",
        "        file.write(f\"{sheet},{row_index},\" + \",\".join(map(str, vector)) + \"\\n\")\n",
        "\n",
        "print(f\"Document vectors with RowIndex saved to {output_file}\")"
      ],
      "id": "7f53f8cc076dd623"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T13:45:57.412346Z",
          "start_time": "2024-12-03T13:45:57.160168Z"
        },
        "id": "c21547d608da067a",
        "outputId": "2becff9b-4799-4d3d-d220-edddec836b24"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "output_file = \"output_files/doc2vec_vectors.csv\"  # Replace with your file path\n",
        "\n",
        "try:\n",
        "    # Read the first 10 rows of the CSV file\n",
        "    data = pd.read_csv(output_file)\n",
        "    print(data.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"File '{output_file}' not found. Please check the file path.\")"
      ],
      "id": "c21547d608da067a",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Sheet  RowIndex      Dim0      Dim1      Dim2      Dim3      Dim4      Dim5  \\\n",
            "0   A-J         0 -0.120381  0.379345  0.134694 -0.177616 -0.337673  0.041946   \n",
            "1   A-J         1 -0.067182  0.138951  0.090573  0.097399  0.135496 -0.669350   \n",
            "2   A-J         2  0.025135  0.409332  0.405838  0.012679 -0.301956 -0.096406   \n",
            "3   A-J         3  0.036234  0.189337  0.372502  0.030741  0.225192 -0.578521   \n",
            "4   A-J         4 -0.228523  0.438873  0.277208  0.343170 -0.130110 -0.245843   \n",
            "\n",
            "       Dim6      Dim7  ...    Dim290    Dim291    Dim292    Dim293    Dim294  \\\n",
            "0  0.782558  0.720419  ...  0.071085  0.264771  0.607232  0.047377  0.579811   \n",
            "1  0.080852  0.947449  ... -0.025642  0.406675  0.322913 -0.005030  0.661448   \n",
            "2  0.210589  0.551659  ...  0.046759  0.205108  0.021800  0.378580  0.208742   \n",
            "3  0.198960  0.444680  ... -0.048156  0.490379  0.326162 -0.107866  0.724978   \n",
            "4  0.018259  0.585959  ...  0.031302  0.473778  0.751093 -0.053052  0.747913   \n",
            "\n",
            "     Dim295    Dim296    Dim297    Dim298    Dim299  \n",
            "0  0.348717 -0.109349 -0.297170  0.090944  0.191706  \n",
            "1  0.231077  0.241409 -0.196228 -0.126254  0.397652  \n",
            "2  0.213977 -0.080312  0.004675  0.004685 -0.248062  \n",
            "3  0.847633  0.372963 -0.118219  0.098637  0.038299  \n",
            "4  0.520868  0.551713 -0.533647  0.727080 -0.115523  \n",
            "\n",
            "[5 rows x 302 columns]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "3c633bc0ce03c0ce"
      },
      "cell_type": "raw",
      "source": [],
      "id": "3c633bc0ce03c0ce"
    },
    {
      "metadata": {
        "id": "de8048c6d164ba48"
      },
      "cell_type": "markdown",
      "source": [
        "Part 6: Ssentence_BERT\n"
      ],
      "id": "de8048c6d164ba48"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T14:40:15.370502Z",
          "start_time": "2024-12-03T14:40:07.456884Z"
        },
        "id": "8391c224f8bba099",
        "outputId": "bcd3fc52-4ca6-4a4c-9284-67ff47960d52"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "id": "8391c224f8bba099",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\Daniel\\PycharmProjects\\IR-exercise\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-03T14:43:24.716355Z",
          "start_time": "2024-12-03T14:41:52.926674Z"
        },
        "id": "dd18ad691ae6613c"
      },
      "cell_type": "code",
      "source": [
        "# Download NLTK resources if needed\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# File path to source documents\n",
        "source_file = \"posts_first_targil.xlsx\"\n",
        "output_file = \"output_files/sbert_vectors.csv\"\n",
        "\n",
        "# Load source documents\n",
        "df = pd.read_excel(source_file, sheet_name=None)\n",
        "if \"J-P\" in df:\n",
        "    df[\"J-P\"].rename(columns={\"Body\": \"Body Text\"}, inplace=True)\n",
        "\n",
        "# Load pre-trained SBERT model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "results = []\n",
        "\n",
        "# Function to process each document and generate a document vector\n",
        "def get_document_vector(text):\n",
        "    # Split the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    # Generate sentence vectors using SBERT\n",
        "    sentence_vectors = model.encode(sentences)\n",
        "    # Average the sentence vectors to get the document vector\n",
        "    document_vector = sentence_vectors.mean(axis=0)\n",
        "    # Normalize the vector by dividing by the number of sentences\n",
        "    document_vector /= len(sentences)\n",
        "    return document_vector\n",
        "\n",
        "for sheet_name, data in df.items():\n",
        "    for index, row in data.iterrows():\n",
        "        if sheet_name == \"A-J\":\n",
        "             combined_text = \" \".join(str(row[col]) for col in ['title', 'sub_title', 'Body Text'] if pd.notna(row[col]))\n",
        "        else:\n",
        "             combined_text = \" \".join(str(row[col]) for col in ['title', 'Body Text'] if pd.notna(row[col]))\n",
        "\n",
        "        # Generate document vector\n",
        "        document_vector = get_document_vector(combined_text)\n",
        "        # Convert to list and append results\n",
        "        results.append([sheet_name, index] + document_vector.tolist())\n",
        "\n",
        "# Save results to CSV\n",
        "header = [\"Sheet\", \"RowIndex\"] + [f\"Dim{i}\" for i in range(len(results[0]) - 2)]\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(\",\".join(header) + \"\\n\")\n",
        "    for row in results:\n",
        "        file.write(\",\".join(map(str, row)) + \"\\n\")\n",
        "\n",
        "print(f\"SBERT vectors saved to {output_file}\")\n"
      ],
      "id": "dd18ad691ae6613c",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "3799357dd521d293"
      },
      "cell_type": "markdown",
      "source": [
        "Part 7: BERT with IDF"
      ],
      "id": "3799357dd521d293"
    },
    {
      "metadata": {
        "id": "3e0cbb911dd3cb07"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n"
      ],
      "id": "3e0cbb911dd3cb07"
    },
    {
      "metadata": {
        "id": "41c3ed487e70439a"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "source_file = \"posts_first_targil.xlsx\"\n",
        "output_file = \"output_files/new_bert_vectors.csv\"\n",
        "\n",
        "df = pd.read_excel(source_file, sheet_name=None)\n",
        "if \"J-P\" in df:\n",
        "    df[\"J-P\"].rename(columns={\"Body\": \"Body Text\"}, inplace=True)\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "def calculate_idf(corpus):\n",
        "    vectorizer = TfidfVectorizer(use_idf=True, stop_words=\"english\")\n",
        "    vectorizer.fit(corpus)\n",
        "    idf_dict = defaultdict(lambda: 0)\n",
        "    for word, idf in zip(vectorizer.get_feature_names_out(), vectorizer.idf_):\n",
        "        idf_dict[word] = idf\n",
        "    return idf_dict\n",
        "\n",
        "corpus = []\n",
        "for sheet_name, data in df.items():\n",
        "    for index, row in data.iterrows():\n",
        "        if sheet_name == 'A-J':\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'sub_title', 'Body Text'] if pd.notna(row[col]))\n",
        "        else:\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'Body Text'] if pd.notna(row[col]))\n",
        "        corpus.append(combined_text)\n",
        "\n",
        "idf_dict = calculate_idf(corpus)\n",
        "\n",
        "\n",
        "def get_bert_vectors(text_chunk):\n",
        "    inputs = tokenizer(text_chunk, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    token_embeddings = outputs.last_hidden_state.squeeze(0)  # Shape: [sequence_length, hidden_size]\n",
        "    attention_mask = inputs[\"attention_mask\"].squeeze(0)  # Shape: [sequence_length]\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze(0))  # List of tokens\n",
        "\n",
        "    return tokens, token_embeddings, attention_mask\n",
        "\n",
        "# Function to process subwords into full word embeddings\n",
        "def process_tokens(tokens, token_embeddings, attention_mask, idf_dict):\n",
        "    word_embeddings = []\n",
        "    current_word = \"\"\n",
        "    current_word_vectors = []\n",
        "\n",
        "    for token, embedding, mask in zip(tokens, token_embeddings, attention_mask):\n",
        "        if mask == 0 or token in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
        "            continue\n",
        "\n",
        "        if token.startswith(\"##\"):  # Subword continuation\n",
        "            current_word += token[2:]\n",
        "            current_word_vectors.append(embedding)\n",
        "        else:  # New word starts\n",
        "            if current_word:  # Combine previous word embeddings\n",
        "                combined_embedding = torch.mean(torch.stack(current_word_vectors), dim=0)\n",
        "                idf = idf_dict.get(current_word, 1.0)  # Default IDF to 1.0 if not found\n",
        "                word_embeddings.append(combined_embedding * idf)\n",
        "\n",
        "            # Start new word\n",
        "            current_word = token\n",
        "            current_word_vectors = [embedding]\n",
        "\n",
        "    # Process the last word\n",
        "    if current_word:\n",
        "        combined_embedding = torch.mean(torch.stack(current_word_vectors), dim=0)\n",
        "        idf = idf_dict.get(current_word, 1.0)\n",
        "        word_embeddings.append(combined_embedding * idf)\n",
        "\n",
        "    return word_embeddings\n",
        "# Function to process an entire document\n",
        "def process_document(text, idf_dict):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    max_tokens = 512\n",
        "    num_chunks = (len(tokens) + max_tokens - 1) // max_tokens  # Ceiling division\n",
        "    all_word_embeddings = []\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        chunk_tokens = tokens[i * max_tokens : (i + 1) * max_tokens]\n",
        "        chunk_text = tokenizer.convert_tokens_to_string(chunk_tokens)\n",
        "        tokens, embeddings, attention_mask = get_bert_vectors(chunk_text)\n",
        "        chunk_word_embeddings = process_tokens(tokens, embeddings, attention_mask, idf_dict)\n",
        "        all_word_embeddings.extend(chunk_word_embeddings)\n",
        "\n",
        "    # Aggregate all word embeddings for the document (e.g., by mean or sum)\n",
        "    document_embedding = torch.mean(torch.stack(all_word_embeddings), dim=0)\n",
        "    return document_embedding\n",
        "\n",
        "\n",
        "results = []\n",
        "for sheet_name, data in df.items():\n",
        "    for index, row in data.iterrows():\n",
        "        if sheet_name == \"A-J\":\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'sub_title', 'Body Text'] if pd.notna(row[col]))\n",
        "        else:\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'Body Text'] if pd.notna(row[col]))\n",
        "\n",
        "        # Generate BERT vectors for the document\n",
        "        bert_vector = process_document(combined_text, idf_dict)\n",
        "        vector_list = bert_vector.tolist()\n",
        "        results.append([sheet_name, index] + vector_list)\n",
        "        print(vector_list)\n",
        "\n",
        "\n",
        "\n",
        "# Save vectors to CSV\n",
        "header = [\"Sheet\", \"RowIndex\"] + [f\"Dim{i}\" for i in range(bert_vector.shape[0])]\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(\",\".join(header) + \"\\n\")\n",
        "    for row in results:\n",
        "        file.write(\",\".join(map(str, row)) + \"\\n\")\n",
        "\n",
        "print(f\"BERT vectors with RowIndex saved to {output_file}\")"
      ],
      "id": "41c3ed487e70439a"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-12-10T08:58:08.898478Z",
          "start_time": "2024-12-10T08:58:08.544064Z"
        },
        "id": "d45b78118b6403e0",
        "outputId": "925682c5-b56c-45d0-be1e-fdc1c4208528"
      },
      "cell_type": "code",
      "source": [
        "output_file = \"output_files/new_bert_vectors.csv\"\n",
        "\n",
        "try:\n",
        "    data = pd.read_csv(output_file)\n",
        "    print(data.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"File '{output_file}' not found. Please check the file path.\")"
      ],
      "id": "d45b78118b6403e0",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Sheet  RowIndex      Dim0      Dim1      Dim2      Dim3      Dim4      Dim5  \\\n",
            "0   A-J         0 -0.788050  0.056614  1.110174 -1.022818  0.147041 -0.474798   \n",
            "1   A-J         1 -0.003478  0.293304 -1.017949 -0.549554  0.197699 -0.491028   \n",
            "2   A-J         2 -0.168909  0.331912  0.046262 -0.966383 -0.039441 -1.237246   \n",
            "3   A-J         3 -0.188200 -0.066150  0.061154 -0.881887  0.318995 -0.174752   \n",
            "4   A-J         4  0.367581  0.150649 -0.167966 -0.829529  0.578362 -0.619802   \n",
            "\n",
            "       Dim6      Dim7  ...    Dim758    Dim759    Dim760    Dim761    Dim762  \\\n",
            "0  0.684777  1.736403  ...  0.441260 -0.295122  1.369656 -0.888012  1.068354   \n",
            "1 -0.230678  0.669581  ...  0.126593  0.276906  0.418110 -0.073043  0.532075   \n",
            "2  0.228654  1.053331  ...  0.301428  0.401214  0.752306 -0.367575  0.183579   \n",
            "3 -0.718530  1.247533  ...  0.775314  0.303543  0.816760 -0.345624 -0.053506   \n",
            "4 -0.128312  0.784007  ...  0.429937  0.090282  0.748448 -0.224150  0.255825   \n",
            "\n",
            "     Dim763    Dim764    Dim765    Dim766    Dim767  \n",
            "0 -1.006198 -0.373084 -0.121401  0.322801  0.021724  \n",
            "1 -1.112056  0.301055 -0.610574 -0.392991  0.147567  \n",
            "2 -1.709214 -0.223334 -1.435423 -0.652594 -0.533352  \n",
            "3 -1.028428 -0.775664 -1.202621 -0.520046  0.887129  \n",
            "4 -1.365864 -0.680425 -1.341126  0.401274  0.005645  \n",
            "\n",
            "[5 rows x 770 columns]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bert Withput IDF"
      ],
      "metadata": {
        "id": "g1zFkCC42sZU"
      },
      "id": "g1zFkCC42sZU"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "source_file = \"/content/output/posts_first_targil.xlsx\"\n",
        "output_file = \"/content/output/bert_withoutIDF.csv\"\n",
        "\n",
        "df = pd.read_excel(source_file, sheet_name=None)\n",
        "if \"J-P\" in df:\n",
        "    df[\"J-P\"].rename(columns={\"Body\": \"Body Text\"}, inplace=True)\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "def get_bert_vectors(text_chunk):\n",
        "    inputs = tokenizer(text_chunk, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    token_embeddings = outputs.last_hidden_state.squeeze(0)  # Shape: [sequence_length, hidden_size]\n",
        "    attention_mask = inputs[\"attention_mask\"].squeeze(0)  # Shape: [sequence_length]\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze(0))  # List of tokens\n",
        "\n",
        "    return tokens, token_embeddings, attention_mask\n",
        "\n",
        "# Function to process subwords into full word embeddings\n",
        "def process_tokens(tokens, token_embeddings, attention_mask):\n",
        "    word_embeddings = []\n",
        "    current_word = \"\"\n",
        "    current_word_vectors = []\n",
        "\n",
        "    for token, embedding, mask in zip(tokens, token_embeddings, attention_mask):\n",
        "        if mask == 0 or token in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
        "            continue\n",
        "\n",
        "        if token.startswith(\"##\"):  # Subword continuation\n",
        "            current_word += token[2:]\n",
        "            current_word_vectors.append(embedding)\n",
        "        else:  # New word starts\n",
        "            if current_word:  # Combine previous word embeddings\n",
        "                combined_embedding = torch.mean(torch.stack(current_word_vectors), dim=0)\n",
        "                word_embeddings.append(combined_embedding)\n",
        "\n",
        "            # Start new word\n",
        "            current_word = token\n",
        "            current_word_vectors = [embedding]\n",
        "\n",
        "    # Process the last word\n",
        "    if current_word:\n",
        "        combined_embedding = torch.mean(torch.stack(current_word_vectors), dim=0)\n",
        "        word_embeddings.append(combined_embedding)\n",
        "\n",
        "    return word_embeddings\n",
        "# Function to process an entire document\n",
        "def process_document(text):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    max_tokens = 512\n",
        "    num_chunks = (len(tokens) + max_tokens - 1) // max_tokens  # Ceiling division\n",
        "    all_word_embeddings = []\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        chunk_tokens = tokens[i * max_tokens : (i + 1) * max_tokens]\n",
        "        chunk_text = tokenizer.convert_tokens_to_string(chunk_tokens)\n",
        "        tokens, embeddings, attention_mask = get_bert_vectors(chunk_text)\n",
        "        chunk_word_embeddings = process_tokens(tokens, embeddings, attention_mask)\n",
        "        all_word_embeddings.extend(chunk_word_embeddings)\n",
        "\n",
        "    # Aggregate all word embeddings for the document (e.g., by mean or sum)\n",
        "    document_embedding = torch.mean(torch.stack(all_word_embeddings), dim=0)\n",
        "    return document_embedding\n",
        "\n",
        "\n",
        "results = []\n",
        "for sheet_name, data in df.items():\n",
        "    for index, row in data.iterrows():\n",
        "        if sheet_name == \"A-J\":\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'sub_title', 'Body Text'] if pd.notna(row[col]))\n",
        "        else:\n",
        "            combined_text = \" \".join(str(row[col]) for col in ['title', 'Body Text'] if pd.notna(row[col]))\n",
        "\n",
        "        # Generate BERT vectors for the document\n",
        "        bert_vector = process_document(combined_text)\n",
        "        vector_list = bert_vector.tolist()\n",
        "        results.append([sheet_name, index] + vector_list)\n",
        "        print(vector_list)\n",
        "\n",
        "\n",
        "\n",
        "# Save vectors to CSV\n",
        "header = [\"Sheet\", \"RowIndex\"] + [f\"Dim{i}\" for i in range(bert_vector.shape[0])]\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(\",\".join(header) + \"\\n\")\n",
        "    for row in results:\n",
        "        file.write(\",\".join(map(str, row)) + \"\\n\")\n",
        "\n",
        "print(f\"BERT vectors with RowIndex saved to {output_file}\")\n",
        "\n",
        "\n",
        "output_file = \"output_files/new_bert_vectors.csv\"\n",
        "\n",
        "try:\n",
        "    data = pd.read_csv(output_file)\n",
        "    print(data.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"File '{output_file}' not found. Please check the file path.\")"
      ],
      "metadata": {
        "id": "Fx-3yJ0C2rSV"
      },
      "id": "Fx-3yJ0C2rSV",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}